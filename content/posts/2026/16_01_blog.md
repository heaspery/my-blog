---
title: "Comment utiliser intelligemment les LLM pour se former au développement web?"
date: 2026-01-14T22:45:37+07:00
slug: /llm-apprentissage/
description: Dans cet article je vous partage mon expérience, les dangers et les techniques que j'ai mises en place afin de garantir un apprentissage sain et efficace du développement web.
image: images/llm.jpeg
caption: Credits - Adobe Stock
categories:
  - Développement
tags:
  - LLM
  - Pédagogie
  - Développement
draft: false
---

## Comment utiliser intelligemment les LLM pour se former dans son domaine ?

L’usage des LLM (Claude, ChatGPT, etc.) est aujourd’hui très présent dans le milieu du numérique. Dans le cadre d’une formation, ils peuvent être à la fois un accélérateur puissant et un danger. La question n’est donc pas “faut-il les utiliser ?”, mais plutôt : comment les utiliser intelligemment pour apprendre réellement, en particulier dans mon domaine : le développement web appliqué aux domaines culturels, sociaux et de médiation scientifique.


## Les dangers pour me former dans mon domaine

Les dangers pour me former avec des LLMs dans mon domaine sont multiples.

Tout d’abord, selon moi, l’utilisation des LLMs pour apprendre à coder n’est pas réaliste si elle est utilisée comme raccourci. Leur utilisation facilite et accélère grandement le développement. Or, il convient d’utiliser les LLMs pour apprendre le développement avec grande précaution. Même si l’on comprend le code généré par un agent, la compréhension ne sera pas aussi profonde que si l’on a écrit le code soi-même.

Selon moi, nous ne devrions jamais prendre pour acquis ce que nous génère un LLM. Il est connu et sûr qu’ils font des erreurs : ils sont susceptibles d’avoir des hallucinations, de nous divulguer des fausses informations, ou de proposer du code de mauvaise qualité, déprécié, voire non adapté à notre contexte. Si l’on ne connaît pas ce que le LLM nous propose, comment pouvons-nous juger de la qualité de ce qu’il nous donne ? Il n’est pas possible d’appliquer son sens critique sur un contenu dont on n’a pas d’expertise. C’est pourquoi, selon moi, générer du contenu pour répondre à des exercices, travaux de groupes, projets académiques, etc. est une mauvaise idée et un danger direct pour l’apprentissage du développement.

À cela s’ajoute un danger plus “invisible” : la dépendance cognitive. Si j’ai toujours un agent pour écrire, corriger, ou expliquer à ma place, je n’entraîne pas ma mémoire, ni mes réflexes de programmation. Je deviens plus rapide à produire mais pas meilleure à comprendre (et encore, ce n'est pas vraiment moi qui produit). L’illusion d’apprentissage peut être très forte : “ça marche donc j’ai compris”, alors que je n’ai fait qu’intégrer une solution sans l’assimiler.

Par contre, demander des explications de code de bonne qualité, tel que celui issu de documentation officielle ou de sources fiables, peut être pratique. Le LLM peut servir de traducteur pédagogique : il simplifie une idée, reformule un paragraphe, ou explicite un concept trop dense.

Cependant, lorsque l’on utilise l’IA pour générer des choses que l’on connaît déjà, que l’on sait implémenter par soi-même, alors nous serons à même de juger ce qui a été généré et d’appliquer notre sens critique. Dans ce cas de figure, son utilisation devient souhaitable, car elle permet d’accélérer le développement. C’est pour cela que je pense que l’utilisation des LLMs diffère entre les profils Seniors et Juniors. Les Seniors savent évaluer ce qui est produit, à l’inverse des Juniors, qui doivent apprendre. Pour apprendre, il faut des sources fiables, ce que les LLMs ne sont pas.

Finalement, cela concerne aussi la partie culturelle / sociale / scientifique de mon domaine. Je ne peux pas me référer aux informations directes de ChatGPT lorsqu’il s’agit ensuite de partager ces informations au grand public. Afin de lutter contre la désinformation, il est absolument nécessaire de baser mes produits de médiation sur des connaissances fiables.


## Quelles techniques pour m’assurer d’apprendre ?

Pour m’assurer d’apprendre, il faut donc que je ne me réfère pas aux LLMs comme source de connaissance. Je dois les utiliser comme un outil d’assistance, pas comme une autorité ni comme une vérité.

Dans mon cas, je considère que le LLM peut être utile si je le place dans l’un de ces rôles :

* **Un outil d’explication** : reformuler la documentation, clarifier un concept, proposer des analogies
* **Un outil de débogage guidé** : aider à comprendre une erreur et ses causes possibles
* **Un outil de feedback** : analyser mon code, relever des incohérences, proposer des améliorations argumentées
* **Un outil d’entraînement** : me poser des questions, me générer des exercices, simuler un entretien technique
* **Un outil de comparaison** : proposer plusieurs approches / architectures et m’aider à choisir en comprenant les trade-offs

Le LLM permet d’accélérer la compréhension de certaines erreurs ou aider à comprendre certains points flous. Mais il ne doit pas être utilisé comme source de connaissance.


## Comment les mettre en œuvre ?

Étant encore Junior et en formation, j’applique les techniques suivantes, qui me permettent d’utiliser l’IA comme aide sans perdre l’apprentissage :

1. **Pour apprendre les bases d’une nouvelle technologie**, je me réfère à de la documentation officielle (et non à un prompt ChatGPT comme point de départ).

2. **Pour développer une application dans un langage nouveau**, je l’écris par moi-même en faisant des itérations, en comprenant les erreurs par moi-même. L’objectif est d’acquérir les réflexes.

3. **Pour expliquer des parties de documentation ou des erreurs que j’ai de la peine à saisir**, je peux m’aider de l’IA si je ne trouve pas d’explication ailleurs. Dans ce cas, je vérifie toujours la réponse avec des sources fiables.

4. **Pour développer des fonctionnalités dans des langages que je maîtrise**, et où l’utilisation de l’IA devient uniquement un vecteur de vitesse et d’efficacité, alors je l’utilise. Mais je garde une logique de contrôle qualité (lecture critique, tests, cohérence globale).

### Exemple concret d’usage

Par exemple, si je dois apprendre une nouvelle librairie front-end :

* Je commence par suivre la doc officielle et un tutoriel de base
* Je code une petite fonctionnalité seule
* Si je bloque sur une erreur, je demande à l’IA d’expliquer le message d’erreur et les causes possibles
* Une fois la fonctionnalité terminée, je supprime une partie du code et je la réécris sans assistance pour vérifier que j’ai réellement compris


## Quel est le constat actuel sur ma formation ?

Ce que je constate dans ma formation, c’est une division assez forte entre les personnes qui ont réellement compris les principes de programmation enseignés, et les autres, qui se sont reposés sur les LLMs pour générer les contenus sans les apprendre en profondeur.

J’ai pu constater que certains camarades de classe qui ne faisaient pas cet effort avaient une compréhension des bases de la programmation très fragile. Souvent, il m’arrive de poser des questions à des collègues qui ont écrit du code dans des travaux de groupe et qui sont incapables de me l’expliquer. Il existe aussi un risque réel d’abus lors des évaluations, car ces outils rendent la triche plus accessible, et cela peut accentuer les disparités de compétences.

Je pense que le problème principal n’est pas seulement moral : il est pédagogique et professionnel. Si certaines personnes ne souhaitent pas travailler dans le développement plus tard, ce manque de compréhension peut sembler “moins grave”. Elles pourront sans doute être capables d’effectuer des tâches en UX design, marketing ou gestion de projet.

Cependant, la valeur du diplôme devient alors plus diluée, et c’est là que les problèmes peuvent survenir. Deux personnes qui sortent de la formation n’auront pas les mêmes compétences, et cela dévalue le travail des personnes qui ont réellement des compétences en développement. La formation pourra perdre de la valeur sur le marché du travail, si des solutions contre la triche et des évaluations plus robustes ne sont pas mises en place.

